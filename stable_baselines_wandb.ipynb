{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W&B and SB3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA1yucrYKx0Q"
      },
      "source": [
        "# RL - W&B and Stable-baselines3 (SB3)\n",
        "\n",
        "Train a Proximal Policy Gradient (PPO) RL model to solve the [Cartpole problem](https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288), a classic intro to RL problem, using OpenAI Gym\n",
        "\n",
        "We will use W&B to track the experiments. At the end, you should see a run page like https://wandb.ai/wandb/cartpole_test/runs/37ppqzxc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLU_dGC8Q2wK"
      },
      "source": [
        "## Sets up dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47bVDhSDPywL"
      },
      "source": [
        "!apt install python-opengl xvfb\n",
        "!pip install pyvirtualdisplay stable_baselines3[extra] wandb\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPlCXGwWFPTI"
      },
      "source": [
        "## Run with SB3 with W&B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U99BWKenKcuv"
      },
      "source": [
        "import gym\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "def make_env():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env = gym.wrappers.Monitor(env, f\"videos\")      # record videos\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env) # record stats suhch as returns\n",
        "    return env\n",
        "\n",
        "experiment_name = \"PPO\"\n",
        "config = {\n",
        "    \"policy\": 'MlpPolicy',\n",
        "    \"total_timesteps\": 25000\n",
        "}\n",
        "wandb.init(\n",
        "    config=config,\n",
        "    sync_tensorboard=True,\n",
        "    project=\"cartpole_test\",\n",
        "    name=experiment_name,\n",
        "    monitor_gym=True,  # automatically upload the videos\n",
        "    save_code=True,\n",
        ")\n",
        "\n",
        "env = DummyVecEnv([make_env])\n",
        "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=f\"runs/{experiment_name}\")\n",
        "model.learn(total_timesteps=25000)\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}