{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_sb3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xVm9QPNVwKXN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/dqn_sb3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgDiMlHXXN7V"
      },
      "source": [
        "# DQN and Double DQN with Stable-Baselines3\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "Double Q-Learning: https://paperswithcode.com/method/double-q-learning\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we will study DQN using Stable-Baselines3 and then see how to reduce value overestimation with double DQN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StmMaKjrX6MC"
      },
      "source": [
        "## Installation\n",
        "\n",
        "We will install master version of SB3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7yLmacAXJ0F"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines3[extra] pyglet==1.5.27"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwC8l-17YseR"
      },
      "source": [
        "Import DQN and evaluation helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYbeqK0tYenp"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xQwxJDgptxH"
      },
      "source": [
        "## The Mountain Car Problem\n",
        "\n",
        "In this environment, the agent must drive an underpowered car up a steep mountain road. For any given state the agent may choose to accelerate to the left, right or cease any acceleration.\n",
        "\n",
        "Source: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
        "\n",
        "![mountaincar.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "    Observation:\n",
        "        Type: Box(2)\n",
        "        Num    Observation               Min            Max\n",
        "        0      Car Position              -1.2           0.6\n",
        "        1      Car Velocity              -0.07          0.07\n",
        "    Actions:\n",
        "        Type: Discrete(3)\n",
        "        Num    Action\n",
        "        0      Accelerate to the Left\n",
        "        1      Don't accelerate\n",
        "        2      Accelerate to the Right\n",
        "        Note: This does not affect the amount of velocity affected by the\n",
        "        gravitational pull acting on the car.\n",
        "    Reward:\n",
        "         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
        "         on top of the mountain.\n",
        "         Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
        "    Starting State:\n",
        "         The position of the car is assigned a uniform random value in\n",
        "         [-0.6 , -0.4].\n",
        "         The starting velocity of the car is always assigned to 0.\n",
        "    Episode Termination:\n",
        "         The car position is more than 0.5\n",
        "         Episode length is greater than 200\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og59Z62aZJna"
      },
      "source": [
        "Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVY05GIhZEMM"
      },
      "source": [
        "env = gym.make(\"MountainCar-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEF4P0DAZMAN"
      },
      "source": [
        "Create the model with tuned hyperparameters from the RL Zoo\n",
        "\n",
        "```yaml\n",
        "MountainCar-v0:\n",
        "  n_timesteps: !!float 1.2e5\n",
        "  policy: 'MlpPolicy'\n",
        "  learning_rate: !!float 4e-3\n",
        "  batch_size: 128\n",
        "  buffer_size: 10000\n",
        "  learning_starts: 1000\n",
        "  gamma: 0.98\n",
        "  target_update_interval: 600\n",
        "  train_freq: 16\n",
        "  gradient_steps: 8\n",
        "  exploration_fraction: 0.2\n",
        "  exploration_final_eps: 0.07\n",
        "  policy_kwargs: \"dict(net_arch=[256, 256])\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbEcqWhqgDmH"
      },
      "source": [
        "tensorboard_log = \"data/tb/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-1scts3Y1c7",
        "outputId": "d8001eb5-3546-4d37-dd8b-32e53c3437c6"
      },
      "source": [
        "dqn_model = DQN(\"MlpPolicy\",\n",
        "            env,\n",
        "            verbose=1,\n",
        "            train_freq=16,\n",
        "            gradient_steps=8,\n",
        "            gamma=0.99,\n",
        "            exploration_fraction=0.2,\n",
        "            exploration_final_eps=0.07,\n",
        "            target_update_interval=600,\n",
        "            learning_starts=1000,\n",
        "            buffer_size=10000,\n",
        "            batch_size=128,\n",
        "            learning_rate=4e-3,\n",
        "            policy_kwargs=dict(net_arch=[256, 256]),\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            seed=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNoFwsCPZQuz"
      },
      "source": [
        "Evaluate the agent before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn0C7RHyZTHA",
        "outputId": "61236eac-6bf0-4eee-b749-251bbd6f9d0c"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:-200.00 +/- 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM0m1NyAgVhR"
      },
      "source": [
        "# Optional: Monitor training in tensorboard\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir $tensorboard_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUoPUfQ4ZexC"
      },
      "source": [
        "We will first train the agent until convergence and then analyse the learned q-value function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wKP0gKjZgWZ"
      },
      "source": [
        "dqn_model.learn(int(1.2e5), log_interval=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI8xh463Zi7M"
      },
      "source": [
        "Evaluate after training, the mean episodic reward should have improved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z952YogYZ8yD",
        "outputId": "5b8f02b6-45cc-4bf0-a47a-54ec0dfce3c2"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward:-100.80 +/- 8.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFiOqKE3aDzI"
      },
      "source": [
        "## Visualize trained agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvVGX13xaGbf",
        "outputId": "61b5d454-7c8f-441e-c3cd-57158043b5d6"
      },
      "source": [
        "record_video('MountainCar-v0', dqn_model, video_length=500, prefix='dqn-mountaincar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video to /content/videos/dqn-mountaincar-step-0-to-step-500.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHaYvk8uaK70"
      },
      "source": [
        "show_videos('videos', prefix='dqn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY00dtf4aRLQ"
      },
      "source": [
        "## Visualize Q-values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP3YH2x7rieM"
      },
      "source": [
        "### Exercise (5 minutes): Retrieve q-values\n",
        "\n",
        "The function will be used to retrieve the learned q-values for a given state (`observation` in the code).\n",
        "\n",
        "The q-network from SB3 DQN can be accessed via `model.q_net` and is a PyTorch module (you can therefore call `.forward()` on it).\n",
        "\n",
        "You need to convert the observation to a PyTorch tensor and then convert the resulting q-values to numpy array.\n",
        "\n",
        "Note: It is recommended to use `with th.no_grad():` context to save computation and memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCp8OpCbKZW"
      },
      "source": [
        "def get_q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Retrieve Q-values for a given observation.\n",
        "\n",
        "    :param model: a DQN model\n",
        "    :param obs: a single observation\n",
        "    :return: the associated q-values for the given observation\n",
        "    \"\"\"\n",
        "    assert model.get_env().observation_space.contains(obs), f\"Invalid observation of shape {obs.shape}: {obs}\"\n",
        "    ### YOUR CODE HERE\n",
        "    # Retrieve q-values for the given observation and convert them to numpy\n",
        "    \n",
        "    ### END OF YOUR CODE\n",
        "    assert isinstance(q_values, np.ndarray), \"The returned q_values is not a numpy array\"\n",
        "    assert q_values.shape == (3,), f\"Wrong shape: (3,) was expected but got {q_values.shape}\"\n",
        "\n",
        "    return q_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcudYVkdbB0e"
      },
      "source": [
        "### Q-values for the initial state\n",
        "\n",
        "Let's reset the environment to start a new episode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izFixcWgaVe3"
      },
      "source": [
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ouWnFaut0KB"
      },
      "source": [
        "we plot the rendered environment to visualize it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "NF1L_1Gfal-g",
        "outputId": "268ac65c-aad7-4626-f7a2-9556d079b60e"
      },
      "source": [
        "plt.axis('off')\n",
        "plt.imshow(env.render(mode=\"rgb_array\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7175798e90>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKIGyJ-bEQO"
      },
      "source": [
        "### Exercise (5 minutes): predict taken action according to q-values\n",
        "\n",
        "Using the `get_q_values()` function, retrieve the q-values for the initial observation, print them for each action (\"left\", \"nothing\", \"right\") and print the action that the greedy (deterministic) policy would follow (i.e., the action with the highest q-value for that state)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPegCYbSuY-f"
      },
      "source": [
        "action_str = [\"Left\", \"Nothing\", \"Right\"]  # action=0 -> go left, action=1 -> do nothing, action=2 -> go right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv90MTHvaqbV"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "# Retrieve q-values for the initial state\n",
        "# You should use `get_q_values()`\n",
        "\n",
        "### END OF YOUR CODE\n",
        "\n",
        "print(f\"Q-value of the initial state left={q_value_left:.2f} nothing={q_value_nothing:.2f} right={q_value_right:.2f}\")\n",
        "\n",
        "### YOUR CODE HERE\n",
        "# Compute the action taken in the initilal state according to q-values \n",
        "# when following a greedy strategy\n",
        "\n",
        "\n",
        "## END of your code here\n",
        "\n",
        "print(f\"Action taken by the greedy policy in the initial state: {action_str[action]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov0UwoBzcaDv"
      },
      "source": [
        "The q-value of the initial state corresponds to how much (discounted) reward the agent expects to get in this episode.\n",
        "\n",
        "We will compare the estimated q-value to the discounted return of the episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhhF-GJccVne"
      },
      "source": [
        "initial_q_value = q_values.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JueeSE1xcQpK"
      },
      "source": [
        "## Step until the end of the episode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_OYobAab8SF"
      },
      "source": [
        "episode_rewards = []\n",
        "done = False\n",
        "i = 0\n",
        "\n",
        "while not done:\n",
        "    i += 1\n",
        "\n",
        "    # Display current state\n",
        "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Retrieve q-value\n",
        "    q_values = get_q_values(dqn_model, obs)\n",
        "\n",
        "    # Take greedy-action\n",
        "    action, _ = dqn_model.predict(obs, deterministic=True)\n",
        "\n",
        "    print(f\"Q-value of the current state left={q_values[0]:.2f} nothing={q_values[1]:.2f} right={q_values[2]:.2f}\")\n",
        "    print(f\"Action: {action_str[action]}\")\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    episode_rewards.append(reward)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBEomET1wkjN"
      },
      "source": [
        "### Exercise (3 minutes): compare estimated initial q-value with actual discounted return\n",
        "\n",
        "Compute the discounted return (sum of discounted reward) of the episode and compare it to the initial estimated q-value.\n",
        "\n",
        "Note: You will need to use the discount factor `dqn_model.gamma`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om4NNW2VdnM9"
      },
      "source": [
        "sum_discounted_rewards = 0\n",
        "\n",
        "### YOUR CODE HERE\n",
        "# Compute the sum of discounted reward for the last episode\n",
        "# using `episode_rewards` list and `dqn_model.gamma` discount factor\n",
        "\n",
        "### END OF YOUR CODE\n",
        "\n",
        "print(f\"Sum discounted rewards: {sum_discounted_rewards:.2f}, initial q-value {initial_q_value:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZloxyPKPg9HX"
      },
      "source": [
        "## Exercise (30 minutes): Double DQN\n",
        "\n",
        "In DQN, the same network is responsible for selecting and estimating the best next action (in the TD-target) and that may lead to over-estimation (the action which q-value is over-estimated will be chosen more often and this slow down training).\n",
        "\n",
        "To reduce over-estimation, double q-learning (and then double DQN) was proposed. It decouples the action selection from the value estimation.\n",
        "\n",
        "Concretely, in DQN, the target q-value is defined as:\n",
        "\n",
        "$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
        "\n",
        "where the target network `q_net_target` with parameters $\\mathbb{\\theta}_{target}$ is used for both action selection and estimation, and can therefore be rewritten:\n",
        "\n",
        "$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right)$$\n",
        "\n",
        "Double DQN uses the online network `q_net` with parameters $\\mathbb{\\theta}_{online}$ to select the action and the target network `q_net_target` to estimate the associated q-values:\n",
        "\n",
        "$$Y^{DoubleDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
        "\n",
        "\n",
        "The goal in this exercise is for you to write the update method for `DoubleDQN`.\n",
        "\n",
        "You will need to:\n",
        "\n",
        "1. Sample replay buffer data using `self.replay_buffer.sample(batch_size)`\n",
        "\n",
        "2. Compute the Double DQN target q-value using the next observations `replay_data.next_observation`, the online network `self.q_net`, the target network `self.q_net_target`, the rewards `replay_data.rewards` and the termination signals `replay_data.dones`. Be careful with the shape of each object ;)\n",
        "\n",
        "3. Compute the current q-value estimates using the online network `self.q_net`, the current observations `replay_data.observations` and the buffer actions `replay_data.actions`\n",
        "\n",
        "4. Compute the loss to train the q-network using L2 or Huber loss (`F.smooth_l1_loss`)\n",
        "\n",
        "\n",
        "Link: https://paperswithcode.com/method/double-q-learning\n",
        "\n",
        "Paper: https://arxiv.org/abs/1509.06461\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4227ILqjg8b4"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class DoubleDQN(DQN):\n",
        "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
        "        # Switch to train mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(True)\n",
        "        # Update learning rate according to schedule\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "\n",
        "        losses = []\n",
        "        for _ in range(gradient_steps):\n",
        "            ### YOUR CODE HERE\n",
        "            # Sample replay buffer\n",
        "            replay_data = ...\n",
        "\n",
        "            # Do not backpropagate gradient to the target network\n",
        "            with th.no_grad():\n",
        "                # Compute the next Q-values using the target network\n",
        "                next_q_values = ...\n",
        "                # Decouple action selection from value estimation\n",
        "                # Compute q-values for the next observation using the online q net\n",
        "                next_q_values_online = ...\n",
        "                # Select action with online network\n",
        "                next_actions_online = ...\n",
        "                # Estimate the q-values for the selected actions using target q network\n",
        "                next_q_values = ...\n",
        "               \n",
        "                # 1-step TD target\n",
        "                target_q_values = ...\n",
        "\n",
        "            # Get current Q-values estimates\n",
        "            current_q_values = ...\n",
        "\n",
        "            # Retrieve the q-values for the actions from the replay buffer\n",
        "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
        "\n",
        "            # Check the shape\n",
        "            assert current_q_values.shape == target_q_values.shape\n",
        "\n",
        "            # Compute loss (L2 or Huber loss)\n",
        "            loss = ...\n",
        "\n",
        "            ### END OF YOUR CODE\n",
        "            \n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Optimize the q-network\n",
        "            self.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Clip gradient norm\n",
        "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "            self.policy.optimizer.step()\n",
        "\n",
        "        # Increase update counter\n",
        "        self._n_updates += gradient_steps\n",
        "\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/loss\", np.mean(losses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3fpWWOg_AS"
      },
      "source": [
        "## Monitoring Evolution of the Estimated q-value\n",
        "\n",
        "\n",
        "Here we create a SB3 callback to over-estimate initial q-values and then monitor their evolution over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLbQ9RhUpMOl"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "\n",
        "class MonitorQValueCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback to monitor the evolution of the q-value\n",
        "    for the initial state.\n",
        "    It allows to artificially over-estimate a q-value for initial states.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_interval: int = 2500):\n",
        "        super().__init__()\n",
        "        self.timesteps = []\n",
        "        self.max_q_values = []\n",
        "        self.sample_interval = sample_interval\n",
        "        n_samples = 512\n",
        "        env = gym.make(\"MountainCar-v0\")\n",
        "        # Sample initial states that will be used to monitor the estimated q-value\n",
        "        self.start_obs = np.array([env.reset() for _ in range(n_samples)])\n",
        "    \n",
        "    def _on_training_start(self) -> None:\n",
        "        # Create overestimation\n",
        "        obs = th.tensor(self.start_obs, device=self.model.device).float()\n",
        "        # Over-estimate going left q-value for the initial states\n",
        "        target_q_values = th.ones((len(obs), 1), device=self.model.device).float() * 100\n",
        "\n",
        "        for _ in range(100):\n",
        "            # Get current Q-values estimates\n",
        "            current_q_values = self.model.q_net(obs)\n",
        "\n",
        "            # Over-estimate going left\n",
        "            current_q_values = th.gather(current_q_values, dim=1, index=th.zeros((len(obs), 1), device=self.model.device).long())\n",
        "\n",
        "            loss = F.mse_loss(current_q_values, target_q_values)\n",
        "\n",
        "            # Optimize the policy\n",
        "            self.model.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.model.policy.optimizer.step()\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Sample q-values\n",
        "        if self.n_calls % self.sample_interval == 0:\n",
        "            # Monitor estimated q-values using current model\n",
        "            obs = th.tensor(self.start_obs, device=self.model.device).float()\n",
        "            with th.no_grad():\n",
        "                q_values = self.model.q_net(obs).cpu().numpy()\n",
        "\n",
        "            self.logger.record(\"train/max_q_value\", float(q_values.max()))\n",
        "            self.timesteps.append(self.num_timesteps)\n",
        "            self.max_q_values.append(q_values.max())\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36dtW1c4xQUG"
      },
      "source": [
        "## Evolution of the q-value with initial over-estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-XTmT6SxdOa"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIo1EDx2xcwZ",
        "outputId": "aac8b11a-e4d6-4278-945e-f48070289f1d"
      },
      "source": [
        "dqn_model = DQN(\"MlpPolicy\",\n",
        "            \"MountainCar-v0\",\n",
        "            verbose=1,\n",
        "            train_freq=16,\n",
        "            gradient_steps=8,\n",
        "            gamma=0.99,\n",
        "            exploration_fraction=0.2,\n",
        "            exploration_final_eps=0.07,\n",
        "            target_update_interval=5000,\n",
        "            learning_starts=1000,\n",
        "            buffer_size=25000,\n",
        "            batch_size=128,\n",
        "            learning_rate=4e-3,\n",
        "            policy_kwargs=dict(net_arch=[256, 256]),\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            seed=102)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'MountainCar-v0'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd62HoWsxfBJ"
      },
      "source": [
        "Define the callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcULdR48xhH5"
      },
      "source": [
        "monitor_dqn_value_cb = MonitorQValueCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z_cFIapxhR7"
      },
      "source": [
        "dqn_model.learn(total_timesteps=int(4e4), callback=monitor_dqn_value_cb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxAGOezBx3GS"
      },
      "source": [
        "### Double DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYNcgKsSegj0"
      },
      "source": [
        "double_q = DoubleDQN(\"MlpPolicy\",\n",
        "            \"MountainCar-v0\",\n",
        "            verbose=1,\n",
        "            train_freq=16,\n",
        "            gradient_steps=8,\n",
        "            gamma=0.99,\n",
        "            exploration_fraction=0.2,\n",
        "            exploration_final_eps=0.07,\n",
        "            target_update_interval=5000,\n",
        "            learning_starts=1000,\n",
        "            buffer_size=25000,\n",
        "            batch_size=128,\n",
        "            learning_rate=4e-3,\n",
        "            policy_kwargs=dict(net_arch=[256, 256]),\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            seed=102)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWhQTCXQyBJZ"
      },
      "source": [
        "monitor_double_q_value_cb = MonitorQValueCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvveywQUkEpW"
      },
      "source": [
        "double_q.learn(int(4e4), log_interval=10, callback=monitor_double_q_value_cb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPqcCnbnyIR5"
      },
      "source": [
        "### Evolution of the max q-value for start states over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNeKzUoaa6_K"
      },
      "source": [
        "plt.figure(figsize=(6, 3), dpi=200)\n",
        "plt.title(\"Evolution of max q-value for start states over time\")\n",
        "plt.plot(monitor_dqn_value_cb.timesteps, monitor_dqn_value_cb.max_q_values, label=\"DQN\")\n",
        "plt.plot(monitor_double_q_value_cb.timesteps, monitor_double_q_value_cb.max_q_values, label=\"Double DQN\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqRixViAv6gT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}